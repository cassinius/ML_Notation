% FWF Proposal Template References


@inproceedings{PageDavid:2012:RelationalLearning,
   year = {2012},
   author = {Page, David and Costa, VÃ­tor Santos and Natarajan, Sriraam and Barnard, Aubrey and Peissig, Peggy and Caldwell, Michael},
   title = {Identifying adverse drug events by relational learning},
   booktitle = {26th AAAI Conference on Artificial Intelligence},
   publisher = {AAAI},
   volume = {2012},
   pages = {1599-1605}
}



@article{Abramson2002,
abstract = {Computational grids that couple geographically distributed resources such as PCs, workstations, clusters, and scientific instruments, have emerged as a next generation computing platform for solving large-scale problems in science, engineering, and commerce. However, application development, resource management, and scheduling in these environments continue to be a complex undertaking. In this article, we discuss our efforts in developing a resource management system for scheduling computations on resources distributed across the world with varying quality of service (QoS). Our service-oriented grid computing system called Nimrod-G manages all operations associated with remote execution including resource discovery, trading, scheduling based on economic principles and a user-defined QoS requirement. The Nimrod-G resource broker is implemented by leveraging existing technologies such as Globus, and provides new services that are essential for constructing industrial-strength grids. We present the results of experiments using the Nimrod-G resource broker for scheduling parametric computations on the World Wide Grid (WWG) resources that span five continents.},
archivePrefix = {arXiv},
arxivId = {cs/0111048},
author = {Abramson, David and Buyya, Rajkumar and Giddy, Jonathan},
doi = {10.1016/S0167-739X(02)00085-7},
eprint = {0111048},
file = {:home/bernd/Desktop/A computational economy for grid computing.pdf:pdf},
isbn = {0167-739X},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Computational economy,Grid computing,Grid scheduling,Nimrod-G broker,Resource management},
number = {8},
pages = {1061--1074},
primaryClass = {cs},
title = {{A computational economy for grid computing and its implementation in the Nimrod-G resource broker}},
volume = {18},
year = {2002}
}

@article{Armbrust2010,
	abstract = {Clearing the clouds away from the true potential and obstacles posed by this computing capability.},
	archivePrefix = {arXiv},
	arxivId = {0521865719 9780521865715},
	author = {Armbrust, Michael and Stoica, Ion and Zaharia, Matei and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel},
	doi = {10.1145/1721654.1721672},
	eprint = {0521865719 9780521865715},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/p50-armbrust.pdf:pdf},
	isbn = {9781605589336},
	issn = {00010782},
	journal = {Communications of the ACM},
	number = {4},
	pages = {50},
	pmid = {11242594},
	title = {{A view of cloud computing}},
	volume = {53},
	year = {2010}
}

@book{Burke:2003:Hyperheuristics,
   year = {2003},
   author = {Burke, Edmund and Kendall, Graham and Newall, Jim and Hart, Emma and Ross, Peter and Schulenburg, Sonia},
   title = {Hyper-Heuristics: An Emerging Direction in Modern Search Technology},
   booktitle = {Handbook of Metaheuristics, Volume 57 International Series in Operations Research and Management Science},
   editor = {Glover, Fred and Kochenberger, GaryA},
   publisher = {Springer},
   address = {New York},
   pages = {457-474},
   abstract = {This chapter introduces and overviews an emerging methodology in search and optimisation. One of the key aims of these new approaches, which have been termed hyperheuristics, is to raise the level of generality at which optimisation systems can operate. An objective is that hyper-heuristics will lead to more general systems that are able to handle a wide range of problem domains rather than current meta-heuristic technology which tends to be customised to a particular problem or a narrow class of problems. Hyper-heuristics are broadly concerned with intelligently choosing the right heuristic or algorithm in a given situation. Of course, a hyper-heuristic can be (often is) a (meta-)heuristic and it can operate on (meta-)heuristics. In a certain sense, a hyper-heuristic works at a higher level when compared with the typical application of meta-heuristics to optimisation problems, i.e., a hyper-heuristic could be thought of as a (meta)-heuristic which operates on lower level (meta-)heuristics. In this chapter we will introduce the idea and give a brief history of this emerging area. In addition, we will review some of the latest work to be published in the field.},
   keywords = {Hyper-heuristic, Meta-heuristic, Optimisation, Search},
   doi = {10.1007/0-306-48056-5_16},
   url = {http://dx.doi.org/10.1007/0-306-48056-5_16}
}

@book{Holzinger:2014:SpringerTextbook,
   year = {2014},
   author = {Holzinger, Andreas},
   title = {Biomedical Informatics: Discovering Knowledge in Big Data},
   publisher = {Springer},
   address = {New York},
   abstract = {This book provides a broad overview of the topic Bioinformatics (medical informatics + biological information) with a focus on data, information and knowledge. From data acquisition and storage to visualization, privacy, regulatory, and other practical and theoretical topics, the author touches on several fundamental aspects of the innovative interface between the medical and computational domains that form biomedical informatics. Each chapter starts by providing a useful inventory of definitions and commonly used acronyms for each topic, and throughout the text, the reader finds several real-world examples, methodologies, and ideas that complement the technical and theoretical background. Also at the beginning of each chapter a new section called key problems, has been added, where the author discusses possible traps and unsolvable or major problems. This new edition includes new sections at the end of each chapter, called future outlook and research avenues, providing pointers to future challenges.},
   doi = {10.1007/978-3-319-04528-3}
}

@book{Holzinger:2011:InformationQuality,
   year = {2011},
   author = {Holzinger, Andreas and Simonic, Klaus-Martin},
   title = {Information Quality in e-Health. Lecture Notes in Computer Science LNCS 7058},
   publisher = {Springer},
   address = {Heidelberg, Berlin, New York},
   abstract = {Medical information systems are already highly sophisticated; however, while computer performance has increased exponentially, human cognitive evolution cannot advance at the same speed. Consequently, the focus on interaction and communication between humans and computers is of increasing importance in medicine and healthcare. The daily actions of medical professionals must be the central concern of any innovation. Simply surrounding and supporting them with new and emerging technologies is not sufficient if these increase rather than decrease the workload. Information systems are a central component of modern knowledge-based medicine and health services; therefore, it is necessary for knowledge management to continually be adapted to the needs and demands of medical professionals within this environment of steadily increasing high-tech medicine. Information processing, in particular its potential effectiveness in modern health services and the optimization ofprocesses and operational sequences, is also of increasing interest. It is particularly important for medical information systems (e.g., hospital information systems and decision support systems) to be designed with the daily schedules, responsibilities and exigencies of the medical professionals in mind. Within the context of this symposium our end users are medical professionals and justifiably expect the software technology to provide a clear benefit: to support them efficiently and effectively in their daily activities. In biomedicine, healthcare, clinical medicine and the life sciences, professional end users are confronted with an increased mass of data. Research in human-computer interaction (HCI) and information retrieval (IR) or knowledge discovery in databases and data mining (KDD) has long been working to develop methods that help users to identify, extract, visualize and understand useful information from these masses of high-dimensional and mostly weakly structured data. HCI and IR/KDD, however, take very different perspectives in tackling this challenge; and historically, they have had little collaboration. Our goal is to combine these efforts to support professionals in interactively analyzing information properties and visualizing the relevant information without becoming overwhelmed. The challenge is to bring HCI and IR/KDD researchers to work together and hence reap the benefits that computer science/informatics can provide to the areas of medicine, healthcare and the life sciences },
   keywords = {Information Quality, Knowledge},
   doi = {10.1007/978-3-642-25364-5}
}


@article{HolzingerEtAl:2014:ResearchChallenges,
   year = {2014},
   author = {Holzinger, Andreas and Dehmer, Matthias and Jurisica, Igor},
   title = {Knowledge Discovery and interactive Data Mining in Bioinformatics - State-of-the-Art, future challenges and research directions},
   journal = {BMC Bioinformatics},
   volume = {15},
   number = {S6},
   pages = {I1},
   abstract = {The life sciences, biomedicine and health care are increasingly turning into a data intensive science. Particularly in bioinformatics and computational biology we face not only increased volume and a diversity of highly complex, multi-dimensional and often weakly-structured and noisy data, but also the growing need for integrative analysis and modeling. Due to the increasing trend towards personalized and precision medicine (P4 medicine: Predictive, Preventive, Participatory, Personalized), biomedical data today results from various sources in different structural dimensions, ranging from the microscopic world, and in particular from the omics world (e.g., from genomics, proteomics, metabolomics, lipidomics, transcriptomics, epigenetics, microbiomics, fluxomics, phenomics, etc.) to the macroscopic world (e.g., disease spreading data of populations in public health informatics). The challenge is not only to extract meaningful information from this data, but to gain knowledge, to discover previously unknown insight, look for patterns, and to make sense of the data. },
   keywords = {Knowledge Discovery, Interactive Data Mining, Bioinformatics, Biomedical Informatics, Data intensive Science},
   doi = {doi:10.1186/1471-2105-15-S6-I1},
   url = {http://www.biomedcentral.com/1471-2105/15/S6/I1}
}

@inproceedings{Halevy:2012:Ecosystem,
	author = {Halevy, Alon Y.},
	title = {Towards an Ecosystem of Structured Data on the Web},
	booktitle = {Proceedings of the 15th International Conference on Extending Database Technology},
	series = {EDBT '12},
	year = {2012},
	isbn = {978-1-4503-0790-1},
	location = {Berlin, Germany},
	pages = {1--2},
	numpages = {2},
	url = {http://doi.acm.org/10.1145/2247596.2247597},
	doi = {10.1145/2247596.2247597},
	acmid = {2247597},
	publisher = {ACM},
	address = {New York, NY, USA}
}

@inproceedings{GuptaEtAl2013ProgressonHalevy,
	year = {2013},
	author = {Gupta, N. and Halevy, A. Y. and Harb, B. and Lam, H. and Hongrae, Lee and Madhavan, J. and Fei, Wu and Cong, Yu},
	title = {Recent progress towards an ecosystem of structured data on the Web},
	booktitle = {Proceedings of the 29th International IEEE Conference on Data Engineering (ICDE)},
	pages = {5-8},
	abstract = {Google Fusion Tables aims to support an ecosystem of structured data on the Web by providing a tool for managing and visualizing data on the one hand, and for searching and exploring for data on the other. This paper describes a few recent developments in our efforts to further the ecosystem.},
	keywords = {Google fusion tables, ecosystem},
	doi = {10.1109/ICDE.2013.6544808},
	url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6544808}
}


@article{Pearce2007,
abstract = {We consider how to maintain the topological order of a directed acyclic graph (DAG) in the presence of edge insertions and deletions. We present a new algorithm and, although this has marginally inferior time complexity compared with the best previously known result, we find that its simplicity leads to better performance in practice. In addition, we provide an empirical comparison against three alternatives over a large number of random DAGs. The results show our algorithm is the best for sparse graphs and, surprisingly, that an alternative with poor theoretical complexity performs marginally better on dense graphs},
author = {Pearce, David J. and Kelly, Paul H. J.},
doi = {10.1145/1187436.1210590},
file = {:home/bernd/Desktop/DynamicTopoSortAlg-JEA-07.pdf:pdf},
issn = {10846654},
journal = {Journal of Experimental Algorithmics},
number = {1},
pages = {1.7},
title = {{A dynamic topological sort algorithm for directed acyclic graphs}},
volume = {11},
year = {2007}
}

@article{Buyya2005,
author = {Buyya, Rajkumar and Venugopal, Srikumar},
file = {:home/bernd/Desktop/GridIntro-CSI2005-gentle.pdf:pdf},
journal = {Computer Society of India},
keywords = {and globus,e-science,grid computing,grid middleware,gridbus},
number = {July},
pages = {9--19},
title = {{A Gentle Introduction to Grid Computing and Technologies}},
year = {2005}
}

@article{Foster2008,
abstract = {Cloud Computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for Cloud Computing and there seems to be no consensus on what a Cloud is. On the other hand, Cloud Computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established Grid Computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast Cloud Computing with Grid Computing from various angles and give insights into the essential characteristics of both.},
archivePrefix = {arXiv},
arxivId = {0901.0131},
author = {Foster, Ian and Zhao, Yong and Raicu, Ioan and Lu, Shiyong},
doi = {10.1109/GCE.2008.4738445},
eprint = {0901.0131},
file = {:home/bernd/Desktop/cloud\_computing\_and\_grid\_computing.pdf:pdf},
isbn = {978-1-4244-2860-1},
journal = {2008 Grid Computing Environments Workshop},
pages = {1--10},
title = {{Cloud Computing and Grid Computing 360-Degree Compared}},
year = {2008}
}

@article{Joshi2005,
author = {Joshi, Mahesh},
file = {:home/bernd/Desktop/grid-computing.pdf:pdf},
number = {April},
pages = {1--17},
title = {{Grid Computing}},
year = {2005}
}

@article{Kang2010,
author = {Kang, U and Horng, Duen},
file = {::},
isbn = {9781450302159},
keywords = {all or part of,belief propagation,gim-v,hadoop,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for},
title = {{Inference of beliefs on billion-scale graphs}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.188.5276},
year = {2010}
}
@article{Pandit2007,
abstract = {Given a large online network of online auction users and their histories of transactions, how can we spot anomalies and auction fraud? This paper describes the design and implementation of NetProbe, a system that we propose for solving this problem. NetProbe models auction users and transactions as a Markov Random Field tuned to detect the suspicious patterns that fraudsters create, and employs a Belief Propagation mechanism to detect likely fraudsters. Our experiments show that NetProbe is both efficient and effective for fraud detection. We report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges, where NetProbe was able to spot fraudulent nodes with over 90\% precision and recall, within a matter of seconds. We also report experiments on a real dataset crawled from eBay, with nearly 700,000 transactions between more than 66,000users, where NetProbe was highly effective at unearthing hidden networks of fraudsters, within a realistic response time of about 6 minutes. For scenarios where the underlying data is dynamic in nature, we propose IncrementalNetProbe, which is an approximate, but fast, variant of NetProbe. Our experiments prove that Incremental NetProbe executes nearly doubly fast as compared to NetProbe, while retaining over 99\% of its accuracy.},
author = {Pandit, Shashank and Chau, DH and Wang, Samuel and Faloutsos, Christos},
doi = {10.1145/1242572.1242600},
file = {::},
isbn = {9781595936547},
journal = {Proceedings of the 16th \ldots},
pages = {210, 201},
title = {{Netprobe: a fast and scalable system for fraud detection in online auction networks}},
url = {http://dx.doi.org/10.1145/1242572.1242600$\backslash$nhttp://dl.acm.org/citation.cfm?id=1242600},
volume = {42},
year = {2007}
}
@article{Felzenszwalb2004,
author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
doi = {10.1023/B:VISI.0000022288.19776.77},
file = {::},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {clustering,graph algorithm,image segmentation,perceptual organization},
month = sep,
number = {2},
pages = {167--181},
title = {{Efficient Graph-Based Image Segmentation}},
url = {http://link.springer.com/10.1023/B:VISI.0000022288.19776.77},
volume = {59},
year = {2004}
}
@article{Lee2012,
abstract = {How can knowing about some categories help us to discover new ones in unlabeled images? Unsupervised visual category discovery is useful to mine for recurring objects without human supervision, but existing methods assume no prior information and thus tend to perform poorly for cluttered scenes with multiple objects. We propose to leverage knowledge about previously learned categories to enable more accurate discovery, and address challenges in estimating their familiarity in unsegmented, unlabeled images. We introduce two variants of a novel object-graph descriptor to encode the 2D and 3D spatial layout of object-level co-occurrence patterns relative to an unfamiliar region and show that by using them to model the interaction between an image???s known and unknown objects, we can better detect new visual categories. Rather than mine for all categories from scratch, our method identifies new objects while drawing on useful cues from familiar ones. We evaluate our approach on several benchmark data sets and demonstrate clear improvements in discovery over conventional purely appearance-based baselines.},
author = {Lee, Yong Jae and Grauman, Kristen},
doi = {10.1109/TPAMI.2011.122},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = feb,
number = {2},
pages = {346--58},
pmid = {21670480},
title = {{Object-graphs for context-aware visual category discovery.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21670480},
volume = {34},
year = {2012}
}
@article{Wang2013,
author = {Wang, Xiaofang and Li, Huibin and Bichot, Charles-Edmond and Masnou, Simon and Chen, Liming},
doi = {10.1109/ICIP.2013.6738828},
file = {::},
isbn = {978-1-4799-2341-0},
journal = {2013 IEEE International Conference on Image Processing},
keywords = {image segmentation,sparse representation},
month = sep,
pages = {4019--4023},
publisher = {Ieee},
title = {{A graph-cut approach to image segmentation using an affinity graph based on ???<inf>0</inf>-sparse representation of features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6738828},
year = {2013}
}
@article{Kraska2013,
author = {Kraska, Tim and Talwalkar, Ameet and Duchi, John and Griffith, Rean and Franklin, Michael J and Jordan, Michael},
file = {::},
title = {{MLbase : A Distributed Machine-learning System}},
year = {2013}
}
@article{Low2010,
abstract = {Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.},
archivePrefix = {arXiv},
arxivId = {1006.4990},
author = {Low, Yucheng and Gonzalez, Joseph and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos and Hellerstein, Joseph M.},
eprint = {1006.4990},
file = {:home/bernd/develop/MScThesis/1.MY\_PAPERS/LOW, Yucheng et al (2014) GraphLab - A New Framework For Parallel Machine Learning.pdf:pdf},
journal = {Conference on Uncertainty in Artificial Intelligence},
title = {{GraphLab: A New Framework for Parallel Machine Learning}},
url = {http://arxiv.org/abs/1006.4990},
year = {2010}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
eprint = {1201.0490},
file = {::},
isbn = {1532-4435},
issn = {15324435},
journal = {\ldots of Machine Learning \ldots},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195$\backslash$nhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Smola2011,
author = {Smola, Alex},
doi = {10.1017/CBO9781139042918},
file = {::},
isbn = {9781139042918},
issn = {9780123814807},
title = {{Scaling Up Machine Learning}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139042918},
year = {2011}
}
@article{Yui2013,
author = {Yui, M and Kojima, I},
file = {::},
journal = {Staff.Aist.Go.Jp},
number = {2},
pages = {2013},
title = {{Hivemall: Hive scalable machine learning library}},
url = {http://staff.aist.go.jp/m.yui/publications/hivemall.pdf},
volume = {91},
year = {2013}
}
@article{Crankshaw2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.3809v2},
author = {Crankshaw, Daniel and Bailis, Peter and Gonzalez, Joseph E and Li, Haoyuan and Zhang, Zhao and Franklin, Michael J and Ghodsi, Ali and Jordan, Michael I and Amplab, U C Berkeley},
eprint = {arXiv:1409.3809v2},
file = {::},
journal = {CIDR (to appear)},
title = {{The Missing Piece in Complex Analytics : Low Latency , Scalable Model Management and Serving with Velox}},
year = {2015}
}
@article{Lessard2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1408.3595v3},
author = {Lessard, Laurent and Dec, O C},
eprint = {arXiv:1408.3595v3},
file = {::},
keywords = {control theory,convex optimization,first-order methods,heavy-ball,integral quadratic con-,method,nesterov,proximal gradient methods,s method,semidefinite programming,straints},
pages = {1--40},
title = {{Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints}},
year = {2014}
}

@article{Low2012,
abstract = {While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.},
archivePrefix = {arXiv},
arxivId = {1204.6078},
author = {Low, Yucheng and Bickson, Danny and Gonzalez, Joseph and Guestrin, Carlos and Kyrola, Aapo and Hellerstein, Joseph M},
doi = {10.14778/2212351.2212354},
eprint = {1204.6078},
file = {::},
issn = {2150-8097},
journal = {Proceedings of the VLDB Endowment},
keywords = {a framework for machine,and data mining in,learning,the cloud,tributed graphlab},
number = {8},
pages = {716--727},
title = {{Distributed GraphLab: a framework for machine learning and data mining in the cloud}},
url = {http://dl.acm.org/citation.cfm?id=2212354},
volume = {5},
year = {2012}
}
@article{Figueiredo2003,
abstract = { We advocate a novel approach to grid computing that is based on a combination of "classic" operating system level virtual machines (VMs) and middleware mechanisms to manage VMs in a distributed environment. The abstraction is that of dynamically instantiated and mobile VMs that are a combination of traditional OS processes (the VM monitors) and files (the VM state). We give qualitative arguments that justify our approach in terms of security, isolation, customization, legacy support and resource control, and we show quantitative results that demonstrate the feasibility of our approach front a performance perspective. Finally, we describe the middleware challenges implied by the approach and an architecture for grid computing using virtual machines.},
author = {Figueiredo, R.J. and Dinda, P.a. and Fortes, J.a.B.},
doi = {10.1109/ICDCS.2003.1203506},
file = {::},
isbn = {0-7695-1920-2},
issn = {1063-6927},
journal = {23rd International Conference on Distributed Computing Systems, 2003. Proceedings.},
title = {{A case for grid computing on virtual machines}},
year = {2003}
}
@article{Rodriguez2009,
abstract = {Virtual machines can greatly simplify grid computing by providing an isolated, well-known environment, while increasing security. Also, they can be used as the base technology to dynamically modify the computing elements of a grid, so providing an adaptive environment. In this paper we present a Grid architecture that allows to dynamically adapt the underlying hardware infrastructure to changing Virtual Organization (VO) demands. The backend of the system is able to provide on-demand virtual worker nodes to existing clusters and integrate them in any Globus-based Grid. In this way, we establish a basis to deploy self-adaptive Grids, which can support different VOs in shared physical infrastructures and dynamically adapt its software configuration. Experimental results on a prototyped testbed show less than a 10\% overall performance loss including the hypervisor overhead.},
author = {Rodr\'{\i}guez, Manuel and Tapiador, Daniel and Font\'{a}n, Javier and Huedo, Eduardo and Montero, Rub\'{e}n S. and Llorente, Ignacio M.},
doi = {10.1007/978-3-642-00955-6\_4},
file = {::},
isbn = {3642009549},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in)},
number = {215605},
pages = {23--32},
title = {{Dynamic provisioning of virtual clusters for grid computing}},
volume = {5415 LNCS},
year = {2009}
}
@article{Sotomayor2007,
author = {Sotomayor, Borja},
year = {2007},
file = {::},
title = {{Virtual Machines for Grid Computing}}
}


@inproceedings{MLTechnicalDebt,
	title = {Machine Learning: The High Interest Credit Card of Technical Debt},
	author  = {D. Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young},
	year  = 2014,
	booktitle = {SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)}
}
@article{GraphExtractPaper,
	author = {Holzinger, Andreas and Malle, Bernd and Giuliani, Nicola},
	doi = {10.1007/978-3-319-09891-3\_50},
	file = {:home/bernd/Dropbox/0.topde/FWF.2014/1.PAPERS.2014/2.WIC.2014/1.PAPER/1v2\_GraphExtractFromImages\_FINAL\_20140525ah.pdf:pdf},
	isbn = {9783319098906},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {data mining,data preprocessing,graph extraction,graph- based algorithms,graphs,image analysis,image content analytics,image segmentation,knowledge discovery},
	pages = {552--563},
	title = {{On graph extraction from image data}},
	volume = {8609 LNAI},
	year = {2014}
}
@article{Tordsson2012,
	abstract = {In the past few years, we have witnessed the proliferation of a heterogeneous ecosystem of cloud providers, each one with a different infrastructure offer and pricing policy. We explore this heterogeneity in a novel cloud brokering approach that optimizes placement of virtual infrastructures across multiple clouds and also abstracts the deployment and management of infrastructure components in these clouds. The feasibility of our approach is evaluated in a high throughput computing cluster case study. Experimental results confirm that multi-cloud deployment provides better performance and lower costs compared to the usage of a single cloud only. ?? 2011 Elsevier B.V. All rights reserved.},
	author = {Tordsson, Johan and Montero, Rub\'{e}n S. and Moreno-Vozmediano, Rafael and Llorente, Ignacio M.},
	doi = {10.1016/j.future.2011.07.003},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/TORDSSON (2012) Cloud brokering mechanisms for optimized placement of virtual machines across multiple providers.pdf:pdf},
	isbn = {0167-739X},
	issn = {0167739X},
	journal = {Future Generation Computer Systems},
	keywords = {Cloud computing,Infrastructure as a Service (IaaS),Interoperability,Scheduling},
	number = {2},
	pages = {358--367},
	publisher = {Elsevier B.V.},
	title = {{Cloud brokering mechanisms for optimized placement of virtual machines across multiple providers}},
	url = {http://dx.doi.org/10.1016/j.future.2011.07.003},
	volume = {28},
	year = {2012}
}
@article{Huang2006,
	abstract = {Virtual machine (VM) technologies are experiencing a resurgence in both industry and research communities. VMs offer many desirable features such as security, ease of management, OS customization, performance isolation, check-pointing, and migration, which can be very beneficial to the performance and the manageability of high performance computing (HPC) applications. However, very few HPC applications are currently running in a virtualized environment due to the performance overhead of virtualization. Further, using VMs for HPC also introduces additional challenges such as management and distribution of OS images.In this paper we present a case for HPC with virtual machines by introducing a framework which addresses the performance and management overhead associated with VM-based computing. Two key ideas in our design are: Virtual Machine Monitor (VMM) bypass I/O and scalable VM image management. VMM-bypass I/O achieves high communication performance for VMs by exploiting the OS-bypass feature of modern high speed interconnects such as Infini-Band. Scalable VM image management significantly reduces the overhead of distributing and managing VMs in large scale clusters. Our current implementation is based on the Xen VM environment and InfiniBand. However, many of our ideas are readily applicable to other VM environments and high speed interconnects.We carry out detailed analysis on the performance and management overhead of our VM-based HPC framework. Our evaluation shows that HPC applications can achieve almost the same performance as those running in a native, non-virtualized environment. Therefore, our approach holds promise to bring the benefits of VMs to HPC applications with very little degradation in performance.},
	author = {Huang, Wei and Liu, Jiuxing and Abali, Bulent and Panda, Dhabaleswar K},
	doi = {10.1145/1183401.1183421},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HUANG, WEI (2006) A Case for High Performance Computing with Virtual Machines.pdf:pdf},
	isbn = {1595932828},
	journal = {Proceedings of the 20th annual international conference on supercomputing ICS 06},
	pages = {125--134},
	title = {{A Case for High Performance Computing with Virtual Machines}},
	year = {2006}
}

@article{Youseff2008,
	abstract = {Progress of research efforts in a novel technology is contingent on having a rigorous organization of its knowledge domain and a comprehensive understanding of all the relevant components of this technology and their relationships. Cloud computing is one contemporary technology in which the research community has recently embarked. Manifesting itself as the descendant of several other computing research areas such as service-oriented architecture, distributed and grid computing, and virtualization, cloud computing inherits their advancements and limitations. Towards the end-goal of a thorough comprehension of the field of cloud computing, and a more rapid adoption from the scientific community, we propose in this paper an ontology of this area which demonstrates a dissection of the cloud into five main layers, and illustrates their interrelations as well as their inter-dependency on preceding technologies. The contribution of this paper lies in being one of the first attempts to establish a detailed ontology of the cloud. Better comprehension of the technology would enable the community to design more efficient portals and gateways for the cloud, and facilitate the adoption of this novel computing approach in scientific environments. In turn, this will assist the scientific community to expedite its contributions and insights into this evolving computing field.},
	author = {Youseff, Lamia and Butrico, Maria and {Da Silva}, Dilma},
	doi = {10.1109/GCE.2008.4738443},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/DA SILVA (2008) Toward a Unified Ontology of Cloud Computing.pdf:pdf},
	isbn = {9781424428601},
	issn = {15347362},
	journal = {Grid Computing Environments Workshop, GCE 2008},
	pmid = {21047737},
	title = {{Toward a unified ontology of cloud computing}},
	year = {2008}
}
@article{Liu2012,
	abstract = {Cloud computing is attracting increasing attention as a means of providing users with fast provisioning of computational and storage resources, elastic scaling, and payas-you-go pricing. The integration of scientific workflows and Cloud computing has the potential to significantly improve resource utilization, processing speed, and user experience. This paper proposes a novel approach for deploying bioinformatics workflows in Cloud environments using Galaxy, a platform for scientific workflows, and Globus Provision, a tool for deploying distributed computing clusters on Amazon EC2. Collectively this combination of tools provides an easy to use, high performance and scalable workflow environment that addresses the needs of data-intensive applications through dynamic cluster configuration, automatic user-defined node provisioning, high speed data transfer, and automated deployment and configuration of domain-specific software. To demonstrate how this approach can be used in practice we present a domain-specific workflow use case and performance evaluation.},
	author = {Liu, Bo and Sotomayor, Borja and Madduri, Ravi and Chard, Kyle and Foster, Ian},
	doi = {10.1109/SC.Companion.2012.131},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/Sotomayor (2012) Deploying Bioinformatics Workflows on Clouds with Galaxy and Globus Provision.pdf:pdf},
	isbn = {9780769549569},
	journal = {Proceedings - 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, SCC 2012},
	keywords = {Cloud computing,Galaxy,Globus provision,Scientific workflow},
	pages = {1087--1095},
	title = {{Deploying Bioinformatics Workflows on clouds with galaxy and globus provision}},
	year = {2012}
}
@article{Sotomayor2009,
	abstract = {Using virtual machines as a resource provisioning mechanism offers multiple benefits, most recently exploited by "infrastructure-as-a-service" clouds, but also poses several scheduling challenges. More specifically, although we can use the suspend/resume/migrate capability of virtual machines to support advance reservation of resources efficiently, by using suspension/resumption as a preemption mechanism, this requires adequately modeling the time and resources consumed by these operations to ensure that preemptions are completed before the start of a reservation. In this work we present a model for predicting various runtime overheads involved in using virtual machines, allowing us to efficiently support advance reservations. We extend our lease management software, Haizea, to use this new model in its scheduling decisions, and we use Haizea with the OpenNebula virtual infrastructure manager so the scheduling decisions will be enacted in a Xen cluster. We present both physical and simulated experimental results showing the degree of accuracy of our model and the long-term effects of variables in our model on several workloads.},
	author = {Sotomayor, Borja and Montero, Rub\'{e}n Santiago and Llorente, Ignacio Mart\'{\i}n and Foster, Ian},
	doi = {10.1109/HPCC.2009.17},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/Sotomayor (2009) Resource leasing and the art of suspending vms.pdf:pdf},
	isbn = {9780769537382},
	journal = {2009 11th IEEE International Conference on High Performance Computing and Communications, HPCC 2009},
	pages = {59--68},
	title = {{Resource leasing and the art of suspending virtual machines}},
	year = {2009}
}
@article{Sotomayor2009b,
	author={Sotomayor, B. and Montero, Ruben S. and Llorente, I.M. and Foster, I.},
	journal={Internet Computing, IEEE},
	title={Virtual Infrastructure Management in Private and Hybrid Clouds},
	year={2009},
	month={Sept},
	volume={13},
	number={5},
	pages={14-22},
	keywords={computer centres;public domain software;scheduling;virtual machines;Haizea;OpenNebula;data center management software;hybrid clouds;infrastructure-as-a-service system;open source virtual infrastructure manager;private clouds;resource lease manager;scheduling;virtual infrastructure management;virtual machines;Clouds;Open source software;Resource management;Resource virtualization;Scheduling;Virtual machining;cloud computing;distributed systems;virtual machines},
	doi={10.1109/MIC.2009.119},
	ISSN={1089-7801}
}
    
    
@article{Liu2014,
	abstract = {Due to the upcoming data deluge of genome data, the need for storing and processing large-scale genome data, easy access to biomedical analyses tools, efficient data sharing and retrieval has presented significant challenges. The variability in data volume results in variable computing and storage requirements, therefore biomedical researchers are pursuing more reliable, dynamic and convenient methods for conducting sequencing analyses. This paper proposes a Cloud-based bioinformatics workflow platform for large-scale next-generation sequencing analyses, which enables reliable and highly scalable execution of sequencing analyses workflows in a fully automated manner. Our platform extends the existing Galaxy workflow system by adding data management capabilities for transferring large quantities of data efficiently and reliably (via Globus Transfer), domain-specific analyses tools preconfigured for immediate use by researchers (via user-specific tools integration), automatic deployment on Cloud for on-demand resource allocation and pay-as-you-go pricing (via Globus Provision), a Cloud provisioning tool for auto-scaling (via HTCondor scheduler), and the support for validating the correctness of workflows (via semantic verification tools). Two bioinformatics workflow use cases as well as performance evaluation are presented to validate the feasibility of the proposed approach. ???? 2014 Elsevier Inc.},
	author = {Liu, Bo and Madduri, Ravi K. and Sotomayor, Borja and Chard, Kyle and Lacinski, Lukasz and Dave, Utpal J. and Li, Jianqiang and Liu, Chunchen and Foster, Ian T.},
	doi = {10.1016/j.jbi.2014.01.005},
	file = {:Users/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/GridComputing/LIU, SOTOMAYOR (2014) Cloud-Based\_Bioinformatics\_Workflow\_Platform\_for\_Large-Scale\_Next-Generation\_Sequencing\_Analysis.pdf:pdf},
	isbn = {1532-0480 (Electronic)
	1532-0464 (Linking)},
	issn = {15320464},
	journal = {Journal of Biomedical Informatics},
	keywords = {Bioinformatics,Cloud computing,Galaxy,Scientific workflow,Sequencing analyses},
	pages = {119--133},
	pmid = {24462600},
	publisher = {Elsevier Inc.},
	title = {{Cloud-based bioinformatics workflow platform for large-scale next-generation sequencing analyses}},
	url = {http://dx.doi.org/10.1016/j.jbi.2014.01.005},
	volume = {49},
	year = {2014}
}


@article{Burke2013,
	abstract = {Hyper-heuristics comprise a set of approaches that are motivated (at least in part) by the goal of automating the design of heuristic methods to solve hard computational search problems. An underlying strategic research challenge is to develop more generally applicable search methodologies. The term hyper-heuristic is relatively new; it was first used in 2000 to describe heuristics to choose heuristics in the context of combinatorial optimisation. However, the idea of automating the design of heuristics is not new; it can be traced back to the 1960s. The definition of hyper-heuristics has been recently extended to refer to a search method or learning mechanism for selecting or generating heuristics to solve computational search problems. Two main hyper-heuristic categories can be considered: heuristic selection and heuristic generation.The distinguishing feature of hyper-heuristics is that they operate on a search space of heuristics (or heuristic components) rather than directly on the search space of solutions to the underlying problem that is being addressed. This paper presents a critical discussion of the scientific literature on hyper-heuristics including their origin and intellectual roots, a detailed account of the main types of approaches, and an overview of some related areas. Current research trends and directions for future research are also discussed.},
	author = {Burke, Edmund K and Gendreau, Michel and Hyde, Matthew and Kendall, Graham and Ochoa, Gabriela and \"{O}zcan, Ender and Qu, Rong},
	doi = {10.1057/jors.2013.71},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HyperHeuristics/BURKE (2013) Hyper-heuristics - a survey of the state of the art.pdf:pdf},
	isbn = {0160-5682},
	issn = {0160-5682},
	journal = {Journal of the Operational Research Society},
	keywords = {combinatorial,evolutionary computation,hyper-heuristics,machine learning,metaheuristics,optimisation,scheduling},
	number = {12},
	pages = {1695--1724},
	title = {{Hyper-heuristics: a survey of the state of the art}},
	url = {http://www.palgrave-journals.com/doifinder/10.1057/jors.2013.71},
	volume = {64},
	year = {2013}
}
@article{Burke2010,
	abstract = {The current state of the art in hyper-heuristic research comprises a set of approaches that share the common goal of automating the design and adaptation of heuristic methods to solve hard computational search problems. The main goal is to produce more generally applicable search methodologies. In this chapter we present and overview of previous categorisations of hyper-heuristics and provide a unified classification and definition which captures the work that is being undertaken in this field. We distinguish between two main hyper-heuristic categories: heuristic selection and heuristic generation. Some representative examples of each category are discussed in detail. Our goal is to both clarify the main features of existing techniques and to suggest new directions for hyper-heuristic research.},
	author = {Burke, Edmund K and Hyde, Matthew and Kendall, Graham and Ochoa, Gabriela and Ozcan, Ender and Woodward, John R},
	doi = {doi:10.1007/978-1-4419-1665-5\_15},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HyperHeuristics/BURKE () A Classification of Hyper-heuristic Approaches.pdf:pdf},
	isbn = {978-1-4419-1663-1},
	issn = {0884-8289},
	journal = {Handbook of Metaheuristics},
	keywords = {genetic algorithms,genetic programming},
	pages = {449--468},
	title = {{A Classification of Hyper-heuristics Approaches}},
	volume = {57},
	year = {2010}
}
@article{Burke2009,
	abstract = {Hyper-heuristics comprise a set of approaches with the common goal of automating the design and tuning of heuristic methods to solve hard computational search problems. Themain goal is to produce more generally applicable search method- ologies. The term hyper-heuristic was coined in the early 2000s to refer to the idea of heuristics to choose heuristics. However, the idea of automating the design of com- bined heuristics can be traced back to the early 1960s. With the incorporation of Genetic Programming into hyper-heuristic research, a new type of hyper-heuristics has emerged that we have termed heuristics to generate heuristics. The distinguishing fea- ture of hyper-heuristics is that they operate on a search space of heuristics (or heuristic components) rather than directly on the search space of solutions to the underlying problem, as is the case with most meta-heuristic approaches. This paper presents a literature survey of hyper-heuristics including their origin and intellectual roots, a de- tailed account of the main types of approaches, and an overview of some related areas. Current research trends and directions for future research are also discussed.},
	author = {Burke, E K and Hyde, Matthew and Kendall, Graham and Ochoa, Gabriela and Ozcan, Ender and Qu, R},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HyperHeuristics/BURKE (2009) A Survey of Hyper-heuristics.pdf:pdf},
	journal = {Computer Science Technical Report No NOTTCS-TR-SUB-0906241418-2747, School of Computer Science and Information Technology University of Nottingham},
	title = {{A survey of hyper-heuristics}},
	url = {http://www.cs.nott.ac.uk/TR/SUB/SUB-0906241418-2747.pdf},
	year = {2009}
}
@article{Neustifter2008,
	author = {Neustifter, Andreas and Algorithmik, Seminar},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HyperHeuristics/hyper\_heuristics\_tuwien.pdf:pdf},
	title = {{Hyper-Heuristics Outline Motivation Hyper-Heuristics Using Hyper-Heuristics for 2D Bin Packing}},
	year = {2008}
}
@article{Ross2005,
	abstract = {The term ???hyper-heuristics??? is fairly new, although the notion has been hinted at in papers from time to time since the 1960s (e.g. Crowston et al., 1963 ). The key idea is to devise new algorithms for solving problems by combining known heuristics in ways that allow each to compensate, to some extent, for the weaknesses of others. They might be thought of as heuristics to choose heuristics . They are methods which work with a search space of heuristics. In this sense, they differ from most applications of metaheuristics (see Glover and Kochenberger, 2003 ) which usually work with search spaces of solutions. One of the main goals of research in this area is to devise algorithms that are fast and exhibit good performance across a whole family of problems, presumably because the algorithms address some shared features of the whole set of problems.},
	author = {Ross, Peter},
	doi = {10.1007/0-387-28356-0\_17},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HyperHeuristics/ross\_edinburgh\_hyper.pdf:pdf},
	isbn = {978-0-387-28356-2},
	journal = {Search Methodologies},
	pages = {529--556},
	title = {{Hyper-Heuristics}},
	url = {http://dx.doi.org/10.1007/0-387-28356-0\_17},
	year = {2005}
}
@article{King2011,
abstract = {The reuse of scientific knowledge obtained from one investigation in another investigation is basic to the advance of science. Scientific investigations should therefore be recorded in ways that promote the reuse of the knowledge they generate. The use of logical formalisms to describe scientific knowledge has potential advantages in facilitating such reuse. Here, we propose a formal framework for using logical formalisms to promote reuse. We demonstrate the utility of this framework by using it in a worked example from biology: demonstrating cycles of investigation formalization [F] and reuse [R] to generate new knowledge. We first used logic to formally describe a Robot scientist investigation into yeast (Saccharomyces cerevisiae) functional genomics [f(1)]. With Robot scientists, unlike human scientists, the production of comprehensive metadata about their investigations is a natural by-product of the way they work. We then demonstrated how this formalism enabled the reuse of the research in investigating yeast phenotypes [r(1) = R(f(1))]. This investigation found that the removal of non-essential enzymes generally resulted in enhanced growth. The phenotype investigation was then formally described using the same logical formalism as the functional genomics investigation [f(2) = F(r(1))]. We then demonstrated how this formalism enabled the reuse of the phenotype investigation to investigate yeast systems-biology modelling [r(2) = R(f(2))]. This investigation found that yeast flux-balance analysis models fail to predict the observed changes in growth. Finally, the systems biology investigation was formalized for reuse in future investigations [f(3) = F(r(2))]. These cycles of reuse are a model for the general reuse of scientific knowledge.},
author = {King, Ross D and Liakata, Maria and Lu, Chuan and Oliver, Stephen G and Soldatova, Larisa N},
doi = {10.1098/rsif.2011.0029},
file = {:home/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/MetaLearning/KING (2011) On the formalization and reuse of scientific research.pdf:pdf},
isbn = {1742-5662 (Electronic)$\backslash$r1742-5662 (Linking)},
issn = {1742-5689},
journal = {Journal of the Royal Society, Interface / the Royal Society},
keywords = {logic,ontology,saccharomyces cerevisiae,semantic web},
number = {63},
pages = {1440--1448},
pmid = {21490004},
title = {{On the formalization and reuse of scientific research.}},
volume = {8},
year = {2011}
}
@article{SmithMiles2008,
abstract = {In this paper we propose a meta-learning inspired framework for analysing the performance of meta-heuristics for optimization problems, and developing insights into the relationships between search space characteristics of the problem instances and algorithm performance. Preliminary results based on several meta-heuristics for well-known instances of the Quadratic Assignment Problem are presented to illustrate the approach using both supervised and unsupervised learning methods.},
author = {Smith-Miles, Kate A},
doi = {10.1109/IJCNN.2008.4634391},
file = {:home/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/AlgorithmSelection/MILES (2008) Towards Insightful Algorithm Selection For Optimisaztion Using Meta-Learning Concepts.pdf:pdf},
isbn = {9781424418213},
issn = {1098-7576},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {4118--4124},
title = {{Towards insightful algorithm selection for optimisation using meta-learning concepts}},
year = {2008}
}
@article{Rice1975,
author = {Rice, John R},
file = {:home/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/HyperHeuristics/RICE (1976) The Algorithm Selection Problem.pdf:pdf},
journal = {Advances in Computers},
pages = {65--117},
title = {{The algorithm selection problem}},
volume = {15},
year = {1975}
}
@article{Hutter2007,
author = {Hutter, Frank and Hoos, Holger H},
file = {:home/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/HyperHeuristics/HUTTER (2007) Automatic Algorithm Configuration based on Local Search.pdf:pdf},
journal = {BMC Bioinformatics},
pages = {1152--1157},
title = {{Automatic Algorithm Configuration based on Local Search}},
year = {2007}
}
@article{Horvitz2001,
abstract = {We describe research and results centering on the construction and use of Bayesian models that can predict the run time of problem solvers. Our efforts are motivated by observations of high variance in the time required to solve instances for several challenging problems. The methods have application to the decision-theoretic control of hard search and reasoning algorithms. We illustrate the approach with a focus on the task of predicting run time for general and domain-specific solvers on a hard class of structured constraint satisfaction problems. We review the use of learned models to predict the ultimate length of a trial, based on observing the behavior of the search algorithm during an early phase of a problem session. Finally, we discuss how we can employ the models to inform dynamic run-time decisions.},
author = {Horvitz, Eric and Ruan, Yongshao and Gomes, Carla and Kautz, Henry and Selman, Bart and Chickering, Max},
doi = {10.1016/S1571-0653(04)00335-X},
file = {:home/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/HyperHeuristics/HORVITZ et al (2001) A Bayesian Approach to Tackling Hard Computational Problems.pdf:pdf},
isbn = {1-55860-800-1},
issn = {15710653},
journal = {Electronic Notes in Discrete Mathematics},
pages = {376--391},
title = {{A Bayesian Approach to Tackling Hard Computational Problems (Preliminary Report)}},
volume = {9},
year = {2001}
}
@article{Holzinger2013,
abstract = {BACKGROUND: Professionals in the biomedical domain are confronted with an increasing mass of data. Developing methods to assist professional end users in the field of Knowledge Discovery to identify, extract, visualize and understand useful information from these huge amounts of data is a huge challenge. However, there are so many diverse methods and methodologies available, that for biomedical researchers who are inexperienced in the use of even relatively popular knowledge discovery methods, it can be very difficult to select the most appropriate method for their particular research problem. RESULTS: A web application, called KNODWAT (KNOwledge Discovery With Advanced Techniques) has been developed, using Java on Spring framework 3.1. and following a user-centered approach. The software runs on Java 1.6 and above and requires a web server such as Apache Tomcat and a database server such as the MySQL Server. For frontend functionality and styling, Twitter Bootstrap was used as well as jQuery for interactive user interface operations. CONCLUSIONS: The framework presented is user-centric, highly extensible and flexible. Since it enables methods for testing using existing data to assess suitability and performance, it is especially suitable for inexperienced biomedical researchers, new to the field of knowledge discovery and data mining. For testing purposes two algorithms, CART and C4.5 were implemented using the WEKA data mining framework.},
author = {Holzinger, Andreas and Zupan, Mario},
doi = {10.1186/1471-2105-14-191},
file = {:C$\backslash$:/Users/Bernd Malle/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/holzinger\_zupan.pdf:pdf},
isbn = {1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC Bioinformatics},
keywords = {data analytics,knowledge discovery,methods},
pages = {191},
pmid = {23763826},
title = {{KNODWAT : A scientific framework application for testing knowledge discovery methods for the biomedical domain}},
url = {http://www.biomedcentral.com/1471-2105/14/191/abstract},
volume = {14},
year = {2013}
}
@article{Pandit2007,
abstract = {Given a large online network of online auction users and their histories of transactions, how can we spot anomalies and auction fraud? This paper describes the design and implementation of NetProbe, a system that we propose for solving this problem. NetProbe models auction users and transactions as a Markov Random Field tuned to detect the suspicious patterns that fraudsters create, and employs a Belief Propagation mechanism to detect likely fraudsters. Our experiments show that NetProbe is both efficient and effective for fraud detection. We report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges, where NetProbe was able to spot fraudulent nodes with over 90\% precision and recall, within a matter of seconds. We also report experiments on a real dataset crawled from eBay, with nearly 700,000 transactions between more than 66,000users, where NetProbe was highly effective at unearthing hidden networks of fraudsters, within a realistic response time of about 6 minutes. For scenarios where the underlying data is dynamic in nature, we propose IncrementalNetProbe, which is an approximate, but fast, variant of NetProbe. Our experiments prove that Incremental NetProbe executes nearly doubly fast as compared to NetProbe, while retaining over 99\% of its accuracy.},
author = {Pandit, Shashank and Chau, DH and Wang, Samuel and Faloutsos, Christos},
doi = {10.1145/1242572.1242600},
isbn = {9781595936547},
journal = {Proceedings of the 16th \ldots},
pages = {210, 201},
title = {{Netprobe: a fast and scalable system for fraud detection in online auction networks}},
url = {http://dx.doi.org/10.1145/1242572.1242600$\backslash$nhttp://dl.acm.org/citation.cfm?id=1242600},
volume = {42},
year = {2007}
}

@article{Raz2002risk,
	title={Risk management, project success, and technological uncertainty},
	author={Raz, Tzvi and Shenhar, Aaron J and Dvir, Dov},
	journal={R\&D Management},
	volume={32},
	number={2},
	pages={101--109},
	year={2002},
	publisher={Wiley Online Library}
}
@article{Perminova2008,
	title={Defining uncertainty in projects--a new perspective},
	author={Perminova, Olga and Gustafsson, Magnus and Wikstr{\"o}m, Kim},
	journal={International Journal of Project Management},
	volume={26},
	number={1},
	pages={73--79},
	year={2008},
	publisher={Elsevier}
}
@book{Holzinger2010ProcessGuide,
	title={Process Guide for Students for Interdisciplinary Work in Computer Science/Informatics.
	Second Edition},
	author={Holzinger, Andreas},
	year={2010},
	address={Norderstedt},
	publisher={BoD}
}

@book{Holzinger2012biomedical,
	title={Biomedical Informatics: Computational Sciences meets Life Sciences},
	author={Holzinger, Andreas},
    year={2012},
    address={Norderstedt},
	publisher={BoD}
}

@ONLINE{AndrewNGCeiling,
      title  = "What part of the pipeline to work on next",
      author = "Andrew Ng",
      url    = "https://class.coursera.org/ml-005/lecture/113",
      Note   = "\url{https://class.coursera.org/ml-005/lecture/113}",
      year   = "2013"
}

@ONLINE{KarpathyConvNetJS,
      title  = "Deep learning in your browser",
      author = "Andrej Karpathy",
      url    = "http://cs.stanford.edu/people/karpathy/convnetjs/",
      Note   = "\url{http://cs.stanford.edu/people/karpathy/convnetjs/}",
      year   = "Last accessed: Sept. 3rd, 2015"
}





% Thank you for your interest to pursue your PhD within the Holzinger Group
% Template modified by Andreas Holzinger 01.02.2015 17:00
% Please treat EACH reference with care, be consistent, complete and overall careful (in German: sorgfÃ¤ltig, even if you are a Bachelor student)
% CONTENT modified by XXXX on XX.XX.2015 XX:XX

%[1]
@incollection{HolzingerMalleGiuliani2014GraphExtraction,
   year = {2014},
   author = {Holzinger, Andreas and Malle, Bernd and Giuliani, Nicola},
   title = {On Graph Extraction from Image Data},
   booktitle = {Brain Informatics and Health, BIH 2014, Lecture Notes in Artificial Intelligence, LNAI 8609},
   editor = {Slezak, Dominik and Peters, James F. and Tan, Ah-Hwee and Schwabe, Lars},
   publisher = {Springer},
   address = {Heidelberg, Berlin},
   pages = {552-563},
   abstract = {Hot topics in knowledge discovery and interactive data mining from natural images include the application of topological methods and machine learning algorithms. For any such approach one needs at first a relevant and robust digital content representation from the image data. However, traditional pixel-based image analysis techniques do not effectively extract, hence represent the content. A very promising approach is to extract graphs from images, which is not an easy task. In this paper we present a novel approach for knowledge discovery by extracting graph structures from natural image data. For this purpose, we created a framework built upon modern Web technologies, utilizing HTML canvas and pure Javascript inside a Web-browser, which is a very promising engineering approach. Following a short description of some popular image classification and segmentation methodologies, we outline a specific data processing pipeline suitable for carrying out future scientific research. A demonstration of our implementation, compared to the results of a traditional watershed transformation performed in Matlab showed very promising results in both quality and runtime, despite some remaining challenges. Finally, we provide a short discussion of a few open problems and outline some of our future research routes.},
   keywords = {data preprocessing, image segmentation, image content analytics, knowledge discovery, data mining},
   doi = {10.1007/978-3-319-09891-3_50},
   url = {https://online.tugraz.at/tug_online/voe_main2.getVollText?pDocumentNr=868952&pCurrPk=80830}
}

%[2]
@incollection{HolzingerEtAl2014OnPCD,
   year = {2014},
   author = {Holzinger, Andreas and Malle, Bernd and Bloice, Marcus and Wiltgen, Marco and Ferri, Massimo and Stanganelli, Ignazio and Hofmann-Wellenhof, Rainer},
   title = {On the Generation of Point Cloud Data Sets: Step One in the Knowledge Discovery Process},
   booktitle = {Interactive Knowledge Discovery and Data Mining in Biomedical Informatics, Lecture Notes in Computer Science, LNCS 8401},
   editor = {Holzinger, Andreas and Jurisica, Igor},
   publisher = {Springer},
   address = {Berlin Heidelberg},
   volume = {8401},
   pages = {57-80},
   abstract = {Computational geometry and topology are areas which have much potential for the analysis of arbitrarily high-dimensional data sets. In order to apply geometric or topological methods one must first generate a representative point cloud data set from the original data source, or at least a metric or distance function, which defines a distance between the elements of a given data set. Consequently, the first question is: How to get point cloud data sets? Or more precise: What is the optimal way of generating such data sets? The solution to these questions is not trivial. If a natural image is taken as an example, we are concerned more with the content, with the shape of the relevant data represented by this image than its mere matrix of pixels. Once a point cloud has been generated from a data source, it can be used as input for the application of graph theory and computational topology. In this paper we first describe the case for natural point clouds, i.e. where the data already are represented by points; we then provide some fundamentals of medical images, particularly dermoscopy, confocal laser scanning microscopy, and total-body photography; we describe the use of graph theoretic concepts for image analysis, give some medical background on skin cancer and concentrate on the challenges when dealing with lesion images. We discuss some relevant algorithms, including the watershed algorithm, region splitting (graph cuts), region merging (minimum spanning tree) and finally describe some open problems and future challenges [Graph-based Data Mining].},
   keywords = {data preprocessing, point cloud data sets, graphs, skin cancer, watershed algorithm, region splitting, graph cuts, region merging, mathematical morphology},
   doi = {10.1007/978-3-662-43968-5_4},
   url = {https://online.tugraz.at/tug_online/voe_main2.getVollText?pDocumentNr=974579&pCurrPk=83005}
}


%[3]
@incollection{PreussEtAl2014TerrainCoverage,
   year = {2014},
   author = {Preuss, Michael and Dehmer, Matthias and Pickl, Stefan and Holzinger, Andreas},
   title = {On Terrain Coverage Optimization by Using a Network Approach for universal Graph-based Data Mining and Knowledge Discovery},
   booktitle = {BIH 2014 Lecture Notes in Artificial Intelligence LNAI 8609 },
   editor = {Slezak, Dominik and Tan, Ah-Hwee and Peters, James F.  and Schwabe, Lars},
   publisher = {Springer},
   address = {Heidelberg, Berlin},
   pages = {564-573},
   abstract = {This conceptual paper discusses a graph-based approach for on-line terrain coverage, which has many important research aspects and a wide range of application possibilities, e.g in multi-agents. Such approaches can be used in different application domains, e.g. in medical image analysis. In this paper we discuss how the graphs are being generated and analyzed. In particular, the analysis is important for improving the estimation of the parameter set for the used heuristic in the field of route planning. Moreover, we describe some methods from quantitative graph theory and outline a few potential research routes.},
   keywords = {graph algorithms, multi agents, quantitative graph theory},
   doi = {10.1007/978-3-319-09891-3_51},
   url = {https://online.tugraz.at/tug_online/voe_main2.getVollText?pDocumentNr=974170&pCurrPk=82989}
}

%[4]
@incollection{Holzinger2014ExtravaganzaTutorial,
   year = {2014},
   author = {Holzinger, Andreas},
   title = {Extravaganza Tutorial on Hot Ideas for Interactive Knowledge Discovery and Data Mining in Biomedical Informatics},
   booktitle = {Brain Informatics and Health, BIH 2014, Lecture Notes in Artificial Intelligence, LNAI 8609},
   editor = {Slezak, Dominik and Tan, Ah-Hwee and Peters, James F and Schwabe, Lars},
   publisher = {Springer},
   address = {Heidelberg, Berlin},
   pages = {502-515},
   abstract = {Hot topics in knowledge discovery and interactive data mining from natural images include the application of topological methods and machine learning algorithms. For any such approach one needs at first a relevant and robust digital content representation from the image data. However, traditional pixel-based image analysis techniques do not effectively extract, hence represent the content. A very promising approach is to extract graphs from images, which is not an easy task. In this paper we present a novel approach for knowledge discovery by extracting graph structures from natural image data. For this purpose, we created a framework built upon modern Web technologies, utilizing HTML canvas and pure Javascript inside a Web-browser, which is a very promising engineering approach. Following a short description of some popular image classification and segmentation methodologies, we outline a specific data processing pipeline suitable for carrying out future scientific research. A demonstration of our implementation, compared to the results of a traditional watershed transformation performed in Matlab showed very promising results in both quality and runtime, despite some remaining challenges. Finally, we provide a short discussion of a few open problems and outline some of our future research routes.},
   keywords = {Knowledge Discovery, Data Mining, HCI-KDD, Graph-based Text Mining, Topological Data Mining, Entropy-based Data Mining},
   doi = {10.1007/978-3-319-09891-3_46},
   url = {https://online.tugraz.at/tug_online/voe_main2.getVollText?pDocumentNr=868952&pCurrPk=80830}
}

%[5]
@incollection{HolzingerOfnerDehmer2014GraphMining,
   year = {2014},
   author = {Holzinger, Andreas and Ofner, Bernhard and Dehmer, Matthias},
   title = {Multi-touch Graph-Based Interaction for Knowledge Discovery on Mobile Devices: State-of-the-Art and Future Challenges},
   booktitle = {Interactive Knowledge Discovery and Data Mining in Biomedical Informatics, Lecture Notes in Computer Science, LNCS 8401},
   editor = {Holzinger, Andreas and Jurisica, Igor},
   publisher = {Springer},
   address = {Berlin Heidelberg},
   pages = {241-254},
   abstract = {Graph-based knowledge representation is a hot topic for some years and still has a lot of research potential, particularly in the advancement in the application of graph-theory for creating benefits in the biomedical domain. Graphs are most powerful tools to map structures within a given data set and to recognize relationships between specific data objects. Many advantages of graph-based data structures can be found in the applicability of methods from network analysis, topology and data mining (e.g. small-world phenomenon, cluster analysis). In this paper we present the state-of-the-art in graph-based approaches for multi-touch interaction on mobile devices and we highlight some open problems to stimulate further research and future developments. This is particularly important in the medical domain, as a conceptual graph analysis may provide novel insights on hidden patterns in data, hence support interactive knowledge discovery.[Graph-based Data Mining]},
   keywords = {Graph Based Interaction, Graph-based Data Mining, Interactive Node-Link Graph Visualization},
   doi = {10.1007/978-3-662-43968-5_14},
   url = {https://online.tugraz.at/tug_online/voe_main2.getVollText?pDocumentNr=974629&pCurrPk=83008}
}

@BOOK{lavrac2006,
  title = {Data Mining and Decision Support: Integration and Collaboration},
  publisher = {Springer Berlin Heidelberg},
  year = {2003},
  editor = {Mladenic, D. and Lavra{\v{c}}, N. and Bohanec, M. and Moyle, S.},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-31314-1_4},
  isbn = {978-3-540-31314-4},
  url = {http://dx.doi.org/10.1007/3-540-31314-1_4}
}

@article{tomar2013,
  title={A survey on Data Mining approaches for Healthcare},
  author={Tomar, Divya and Agarwal, Sonali},
  journal={International Journal of Bio-Science and Bio-Technology},
  volume={5},
  number={5},
  pages={241-266},
  year={2013}
}

@ARTICLE{findley1996,
  author = {Findley, Leslie J.},
  title = {Classification of Tremors},
  journal = {Journal of Clinical Neurophysiology},
  year = {1996},
  volume = {13},
  pages = {122-132},
}

@ARTICLE{budzianowska2008,
  author = {Budzianowska, A. and Honczarenko, K.},
  title = {Assessment of rest tremor in Parkinson's disease},
  journal = {Neurol. Neurochir. Pol.},
  year = {2008},
  volume = {42},
  pages = {12-21},
}

@ARTICLE{jankovic2008,
  author = {Jankovic J.},
  title = {Parkinson's disease: clinical features and diagnosis},
  journal = {Journal of Neurology, Neurosurgery Psychiatry},
  year = {2008},
  volume = {79},
  pages = {368-376},
}

@ARTICLE{timmer1993,
  author = {Timmer, J. and Gantert, C. and Deuschl, G. and Honerkamp, J.},
  title = {Characteristics of hand tremor time series},
  journal = {Biological Cybernetics},
  year = {1993},
  volume = {70},
  pages = {75-80}
}

@ARTICLE{riviere1997,
  author = {Riviere, C.N. and Reich, S.G. and Thakor, N.V.},
  title = {Adaptive Fourier modelling for quantification of tremor},
  journal = {J. Neurosci. Methods},
  year = {1997},
  volume = {74},
  pages = {77-87},
}

@INPROCEEDINGS{patel2008,
  author = {Patel, S. and Hughes, R. and Huggins, N. and Standaert, D. and Growdon,
	J. and Dy, J. and Bonato, P.},
  title = {Using wearable sensors to predict the severity of symptoms and motor
	 complications  in  late  stage  Parkinson's  Disease},
  booktitle = {Conf Proc IEEE Eng Med Biol Soc},
  year = {2008},
  pages = {3686-3689},
}
@ARTICLE{patel2009,
  author = {Patel S, Lorincz K, Hughes R,Huggins N, Growdon J, Standaert D, Akay
	M, Dy J, Welsh M, Bonato P.},
  title = {Monitoring motor fluctuations in patients with Parkinson's disease
	using wearable sensors},
  journal = {IEEE Trans Inf Technol Biomed.},
  year = {2009},
  volume = {13},
  pages = {864-873},
}

@ARTICLE{rigas2012,
  author = {Rigas G, Tzallas AT, Tsipouras MG, Bougia P, Tripoliti EE, Baga D,
	Fotiadis DI, Tsouli SG, Konitsiotis S.},
  title = {Assessment of tremor activity in the Parkinson's disease using a
	set of wearable sensors},
  journal = {IEEE Trans Inf Technol Biomed.},
  year = {2012},
  volume = {16},
  pages = {478-487},
}

@ARTICLE{wu2010,
  author = {Wu, D. and Warwick, K. and Ma, Z. and Burgess, J.G. and Pan, S. and
	Aziz, T.Z.},
  title = {Prediction of Parkinson's disease tremor onset using radial basis
	function neural networks},
  journal = {Expert Systems with Applications},
  year = {2010},
  volume = {37},
  pages = {2923-2928},
}

@article{Muniz:2010:JBiomech,
   year = {2010},
   author = {Muniz, AMS and Liu, H and Lyons, KE and Pahwa, R and Liu, W and Nobre, FF and Nadal, J},
   title = {Comparison among probabilistic neural network, support vector machine and logistic regression for evaluating the effect of subthalamic stimulation in Parkinson disease on ground reaction force during gait},
   journal = {Journal of Biomechanics},
   volume = {43},
   number = {4},
   pages = {720-726},

}


@ARTICLE{tahir2012,
  author = {Tahir NMd, Manap HH.},
  title = {Parkinson Disease Gait Classification based on Machine Learning Approach},
  journal = {Journal of Applied Sciences},
  year = {2012},
  volume = {12},
  pages = {180-185},
}

@ARTICLE{bloem2004,
  author = {Bloem BR, Hausdor JM, Visser JE, Giladi N.},
  title = {Falls and freezing of gait in Parkinson's disease: a review of two
	interconnected, episodic phenomena},
  journal = {Mov. Disord.},
  year = {2004},
  volume = {19},
  pages = {871-884},
}

@ARTICLE{giladi2009,
  author = {Giladi N, Tal J, Azulay T, Rascol O, Brooks DJ, Melamed E, Oertel
	W, Poewe WH, Stocchi F, Tolosa E.},
  title = {Validation of the freezing of gait questionnaire in patients with
	Parkinson's disease},
  journal = {Mov. Disord.},
  year = {2009},
  volume = {24},
  pages = {655-661},
}
@ARTICLE{Nieuwboer2004,
  author = {Nieuwboer A, Dom R, De Weerdt W, Desloovere K, Janssens L, Stijn
	V.},
  title = {Electromyographic profiles of gait prior to onset of freezing episodes
	in patients with Parkinson's disease},
  journal = {Brain},
  year = {2004},
  volume = {127},
  pages = {1650-1660},
}

@ARTICLE{delval2010,
  author = {Delval A,  Snijders AH, Weerdesteyn V, Duysens JE, Defebvre L, Giladi
	N, Bloem BR.},
  title = {Objective detection of subtle freezing of gait episodes in Parkinson's
	disease},
  journal = {Mov. Disord.},
  year = {2010},
  volume = {25},
  pages = {1684-1693},
}

@ARTICLE{hausdor2003,
  author = {Hausdor JM, Schaafsma JD, Balash Y, Bartels AL, Gurevich T, Giladi
	N.},
  title = {Impaired regulation of stride variability in Parkinson's disease
	subjects with freezing of gait},
  journal = {Exp Brain Res.},
  year = {2003},
  volume = {149},
  pages = {187-194},
}

@ARTICLE{tripoliti2013,
  author = {Tripoliti EE, Tzallas AT, Tsipouras MG, Rigas G, Bougia P, Leontiou
	M, Konitsiotis S, Chondrogiorgi M, Tsouli S, Fotiadis DI.},
  title = {Automatic detection of freezing of gait events in patients with Parkinson's
	disease},
  journal = {Comput Methods Programs Biomed.},
  year = {2013},
  volume = {110},
  pages = {12-26},
}

@INPROCEEDINGS{han2003,
  author = {Han JH, Lee WJ, Ahn TB, Jeon BS, Park KS.},
  title = {Gait analysis for freezing detection in patients with movement disorder
	using three dimensional acceleration system.},
  booktitle = {Conf Proc IEEE Eng Med Biol Soc.},
  year = {2003},
  volume = {2},
  pages = {1863-1865.},
}

@ARTICLE{bachlin2010,
  author = {B\"achlin M, Plotnik M, Roggen D, Giladi N, Hausdor JM, Tr\"oster
	G.},
  title = {A  wearable  system  to  assist  walking  of  Parkinson  s  disease
	patients},
  journal = {Methods Inf Med.},
  year = {2010},
  volume = {49},
  pages = {88-95},
}

@ARTICLE{muniz2012,
  author = {Muniz AM, Nadal J, Lyons KE, Pahwa R, Liu W.},
  title = {Long-term evaluation of gait initiation in six Parkinson's disease
	patients with bilateral subthalamic stimulation},
  journal = {Gait Posture},
  year = {2012},
  volume = {35},
  pages = {452-457},
}

@ARTICLE{little2009,
  author = {Little MA, McSharry PE, Hunter EJ, Spielman J, Ramig LO.},
  title = {Suitability of dysphonia measurements for telemonitoring of Parkinsonâs
	disease},
  journal = {IEEE Transactions on  Biomedical Engineering},
  year = {2009},
  volume = {56},
  pages = {1015-1022},
}

@ARTICLE{das2010,
  author = {Das R.},
  title = {A comparison of multiple classification methods for diagnosis of
	Parkinson disease},
  journal = {Expert Systems with Applications},
  year = {2010},
  volume = {37},
  pages = {1568-1572},
}
@ARTICLE{eskidere2012,
  author = {Eskidere Ã, ErtaÅ F, HanilÃ§i C.},
  title = {A comparison of regression methods for remote tracking of Parkinsonâs
	disease progression.},
  journal = {Expert Systems with Applications},
  year = {2012},
  volume = {39},
  pages = {5523-5528},
}
@ARTICLE{chen2013,
  author = {Chen HL, Huang CC, Yu XG, Xu X, Sun X, Wang G, Wang SJ.},
  title = {An efficient diagnosis system for detection of Parkinsonâs disease
	using fuzzy k-nearest neighbor approach},
  journal = {Expert Systems with Applications},
  year = {2013},
  volume = {40},
  pages = {263-271},
}
@ARTICLE{little2007,
  author = {Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM.},
  title = {Exploiting nonlinear recurrence and fractal scaling properties for
	voice disorder detection},
  journal = {BioMed Eng OnLine},
  year = {2007},
  volume = {6},
  pages = {23},
}
@ARTICLE{tsanas2010,
  author = {Tsanas A, Little MA, McSharry PE, Ramig LO.},
  title = {Accurate telemonitoring of Parkinson's disease progression by noninvasive
	speech tests},
  journal = {IEEE Trans Biomed Eng.},
  year = {2010},
  volume = {57},
  pages = {884-893},
}
@ARTICLE{li2011,
  author = {Li DC, Liu CW, Hu SC.},
  title = {A fuzzy-based data transformation for feature extraction to increase
	classification performance with small medical data sets},
  journal = {Artificial Intelligence in Medicine},
  year = {2011},
  volume = {52},
  pages = {45-52},
}

@ARTICLE{exarchos2012,
  author = {Exarchos TP, Tzallas AT, Baga D, Chaloglou D, Fotiadis DI, Tsouli
	S, et al.},
  title = {Using partial decision trees to predict Parkinsonâs symptoms: A new
	approach for diagnosis and therapy in patients suffering from Parkinsonâs
	disease},
  journal = {Computers in Biology and Medicine},
  year = {2012b},
  volume = {42},
  pages = {195â204},
}


